{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A_vxjPIvJ2L"
   },
   "source": [
    "# TinyTPU: A simple JAX simulator\n",
    "\n",
    "This notebook implements a simple cycle-accurate TPU simulation. The main goal is to illustrate how simple high-level JAX code can get executed with low-level systolic hardware.\n",
    "\n",
    "It uses eager execution (interpreting operations one-by-one) rather than XLA-style graph compilation.\n",
    "\n",
    "**Architecture**\n",
    "\n",
    "*   Frontend (Interpreter): Intercepts JAX calls (dot, add, relu) at runtime and dispatches them immediately to the simulator.\n",
    "*   Middleware (Tiling): Slices large matrices to fit the hardware grid and handles data skewing required for systolic data flow.\n",
    "*   Backend (Hardware Model): Simulates a Systolic Array (weight-stationary) for matrix math and a Vector Unit for element-wise ops.\n",
    "*   Quantization: Rounds inputs to Int8 before computation to model realistic hardware precision loss.\n",
    "\n",
    "**Usage**: Run the cells sequentially. The final demo executes a complete Dense Layer ($Y = \\text{ReLU}(X \\cdot W + B)$) on the simulator and visualizes the quantization error relative to the FP32 reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGkbCT8HAhFN"
   },
   "outputs": [],
   "source": [
    "# @title Hardware simulator: MxU and VPU\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# --- Configuration ---\n",
    "@dataclass\n",
    "class Config:\n",
    "    grid_size: int = 4\n",
    "    input_width: int = 8\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# --- Hardware Model ---\n",
    "\n",
    "class PE:\n",
    "    \"\"\"Weight-stationary processing element.\"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "        self.weight = 0.0\n",
    "        self.a_in = 0.0  # Activations (Left -> Right)\n",
    "        self.b_in = 0.0  # Partial Sums (Top -> Bottom)\n",
    "        self.a_out = 0.0\n",
    "        self.b_out = 0.0\n",
    "\n",
    "    def load_weight(self, w):\n",
    "        # In a real int8 implementation, we'd clamp here\n",
    "        self.weight = w\n",
    "\n",
    "    def compute(self):\n",
    "        # Basic MAC operation\n",
    "        # For quantization sim, we round inputs before the multiply\n",
    "        w_q = np.round(self.weight)\n",
    "        a_q = np.round(self.a_in)\n",
    "\n",
    "        self.a_out = self.a_in # Pass activation through\n",
    "        self.b_out = self.b_in + (a_q * w_q)\n",
    "\n",
    "    def reset(self):\n",
    "        self.a_in = self.b_in = self.a_out = self.b_out = 0.0\n",
    "\n",
    "class SystolicArray:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.grid = [[PE(x, y) for x in range(size)] for y in range(size)]\n",
    "\n",
    "    def load_weights(self, weights):\n",
    "        assert weights.shape == (self.size, self.size)\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                self.grid[r][c].load_weight(weights[r, c])\n",
    "\n",
    "    def step(self, input_slice):\n",
    "        # Capture current state to simulate synchronous clock edge\n",
    "        # We need to read from neighbors' OUTPUT registers before they update\n",
    "        next_a = np.zeros((self.size, self.size))\n",
    "        next_b = np.zeros((self.size, self.size))\n",
    "\n",
    "        # Read phase\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                # Inputs from left\n",
    "                if c == 0: val_a = input_slice[r]\n",
    "                else:      val_a = self.grid[r][c-1].a_out\n",
    "\n",
    "                # Inputs from top\n",
    "                if r == 0: val_b = 0.0\n",
    "                else:      val_b = self.grid[r-1][c].b_out\n",
    "\n",
    "                # Update PE registers\n",
    "                self.grid[r][c].a_in = val_a\n",
    "                self.grid[r][c].b_in = val_b\n",
    "\n",
    "        # Execute phase\n",
    "        for row in self.grid:\n",
    "            for pe in row:\n",
    "                pe.compute()\n",
    "\n",
    "    def read_out(self):\n",
    "        # Read partial sums flowing out the bottom\n",
    "        return np.array([pe.b_out for pe in self.grid[-1]])\n",
    "\n",
    "class VectorUnit:\n",
    "    \"\"\"\n",
    "    Simulates the TPU's Vector Processing Unit (VPU).\n",
    "    Handles element-wise operations like Add, Relu, Exp.\n",
    "    \"\"\"\n",
    "    def execute(self, opcode, *operands):\n",
    "        # In a cycle-accurate sim, we would model vector lanes here.\n",
    "        # For this functional sim, we perform the operation and quantize the result.\n",
    "\n",
    "        if opcode == 'add':\n",
    "            res = operands[0] + operands[1]\n",
    "        elif opcode == 'relu':\n",
    "            res = np.maximum(operands[0], 0)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"VPU opcode '{opcode}' not supported\")\n",
    "\n",
    "        # Simulating Int8 quantization noise for VPU ops\n",
    "        return np.round(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QPX-M1fqA3_A"
   },
   "outputs": [],
   "source": [
    "# @title Data layout + tiling utilities\n",
    "\n",
    "def skew_matrix(matrix):\n",
    "    # Skews input A so that row 'i' is delayed by 'i' cycles\n",
    "    # Essential for aligning data with the partial sum wavefront\n",
    "    rows, cols = matrix.shape\n",
    "    width = cols + rows + 10 # Buffer for drain time\n",
    "    buffer = np.zeros((rows, width))\n",
    "\n",
    "    for r in range(rows):\n",
    "        buffer[r, r : r + cols] = matrix[r, :]\n",
    "    return buffer\n",
    "\n",
    "def deskew_result(stream, rows, cols):\n",
    "    # Reconstructs the 2D matrix from the temporal output stream\n",
    "    res = np.zeros((rows, cols))\n",
    "    latency = 3 # Hardcoded for 4x4 grid (N-1)\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            # The result for (r, c) appears at specific tick\n",
    "            tick = r + latency + c\n",
    "            if tick < len(stream):\n",
    "                res[r, c] = stream[tick][c]\n",
    "    return res\n",
    "\n",
    "def tile_matmul(lhs, rhs, grid_size):\n",
    "    # Breaks large matmul into grid-sized chunks\n",
    "    # Assumes shapes are divisible by grid_size for now\n",
    "    m, k = lhs.shape\n",
    "    k2, n = rhs.shape\n",
    "    assert k == k2\n",
    "\n",
    "    ops = []\n",
    "\n",
    "    # Iterate through output blocks\n",
    "    for r in range(0, m, grid_size):\n",
    "        for c in range(0, n, grid_size):\n",
    "            # Accumulate along the K dimension\n",
    "            for i in range(0, k, grid_size):\n",
    "                tile_a = lhs[r:r+grid_size, i:i+grid_size]\n",
    "                tile_b = rhs[i:i+grid_size, c:c+grid_size]\n",
    "\n",
    "                ops.append({\n",
    "                    'a': tile_a,\n",
    "                    'b': tile_b,\n",
    "                    'dest': (r, c) # Top-left coordinate in result\n",
    "                })\n",
    "    return ops\n",
    "\n",
    "# --- Execution Engine ---\n",
    "\n",
    "def run_tpu_job(lhs, rhs, grid_size=4):\n",
    "    program = tile_matmul(lhs, rhs, grid_size)\n",
    "\n",
    "    # Pre-allocate output memory\n",
    "    out_shape = (lhs.shape[0], rhs.shape[1])\n",
    "    global_mem = np.zeros(out_shape)\n",
    "\n",
    "    # Init hardware\n",
    "    chip = SystolicArray(grid_size)\n",
    "\n",
    "    for i, op in enumerate(program):\n",
    "        # Reset PEs for new tile\n",
    "        # In real hw we'd stream this, but reset is cleaner for sim\n",
    "        for row in chip.grid:\n",
    "            for pe in row: pe.reset()\n",
    "\n",
    "        chip.load_weights(op['b'])\n",
    "\n",
    "        # Format input (Must transpose A tile for weight-stationary alignment)\n",
    "        # Input A rows need to hit PE columns\n",
    "        inp_stream = skew_matrix(op['a'].T)\n",
    "\n",
    "        # Run\n",
    "        raw_out = []\n",
    "        for t in range(inp_stream.shape[1]):\n",
    "            chip.step(inp_stream[:, t])\n",
    "            raw_out.append(chip.read_out())\n",
    "\n",
    "        # Write back to memory\n",
    "        partial = deskew_result(np.array(raw_out), grid_size, grid_size)\n",
    "        r, c = op['dest']\n",
    "        global_mem[r:r+grid_size, c:c+grid_size] += partial\n",
    "\n",
    "    return global_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kXsRTemqA59a"
   },
   "outputs": [],
   "source": [
    "# @title Dispatcher\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Initialize hardware units\n",
    "vpu = VectorUnit()\n",
    "\n",
    "def tpu_call(func, *args):\n",
    "    \"\"\"\n",
    "    Traffic Controller: Routes JAX ops to the correct hardware unit.\n",
    "    \"\"\"\n",
    "    arrs = [np.array(x) for x in args]\n",
    "\n",
    "    # 1. Matrix Multiplication -> Systolic Array (MXU)\n",
    "    if func.__name__ == 'dot':\n",
    "        print(f\"[Dispatcher] Routing {arrs[0].shape} x {arrs[1].shape} to Systolic Array...\")\n",
    "        return run_tpu_job(arrs[0], arrs[1], config.grid_size)\n",
    "\n",
    "    # 2. Addition -> Vector Unit (VPU)\n",
    "    elif func.__name__ == 'add':\n",
    "        print(f\"[Dispatcher] Routing 'add' to Vector Unit...\")\n",
    "        return vpu.execute('add', arrs[0], arrs[1])\n",
    "\n",
    "    # 3. ReLU -> Vector Unit (VPU)\n",
    "    # JAX doesn't have a simple 'relu' primitive in numpy, so we detect the function name\n",
    "    elif func.__name__ == 'relu':\n",
    "        print(f\"[Dispatcher] Routing 'relu' to Vector Unit...\")\n",
    "        return vpu.execute('relu', arrs[0])\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Op '{func.__name__}' not supported on TinyTPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxmo2EhBuaI1"
   },
   "outputs": [],
   "source": [
    "# @title Demo: Running a dense layer\n",
    "print(\"\\n--- Running Dense Layer (Relu(XW + B)) ---\")\n",
    "\n",
    "# 1. Setup Weights & Biases\n",
    "key = jax.random.PRNGKey(0)\n",
    "k1, k2, k3 = jax.random.split(key, 3)\n",
    "\n",
    "X = jax.random.uniform(k1, (8, 8)) * 5\n",
    "W = jax.random.uniform(k2, (8, 8)) * 5\n",
    "B = jax.random.uniform(k3, (8, 8)) * 5\n",
    "\n",
    "# 2. Execute Eagerly on Simulator\n",
    "# The simulator resets between steps, but data flows conceptually from MXU -> VPU\n",
    "step1 = tpu_call(jnp.dot, X, W)   # Matrix Multiply\n",
    "step2 = tpu_call(jnp.add, step1, B) # Bias Add\n",
    "output = tpu_call(jax.nn.relu, step2) # Activation\n",
    "\n",
    "print(\"\\nFinal Output Snapshot (Top Left):\")\n",
    "print(output[:4, :4])\n",
    "\n",
    "# 3. Verify\n",
    "ref = jax.nn.relu(jnp.add(jnp.dot(X, W), B))\n",
    "mse = np.mean((output - ref)**2)\n",
    "print(f\"\\nLayer MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "EIYbUq5DEHIe"
   },
   "outputs": [],
   "source": [
    "# @title Visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_results(reference, tpu_output, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Visualizes the Reference vs. TPU output and the quantization error.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Define the three plots we want\n",
    "    plots = [\n",
    "        (reference, f\"{title_prefix} Reference (JAX)\", \"viridis\"),\n",
    "        (tpu_output, f\"{title_prefix} TinyTPU (Int8)\", \"viridis\"),\n",
    "        (reference - tpu_output, \"Quantization Error (Diff)\", \"coolwarm\")\n",
    "    ]\n",
    "\n",
    "    for ax, (data, title, cmap) in zip(axes, plots):\n",
    "        im = ax.imshow(data, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        # Magic numbers to make the colorbar match the plot height\n",
    "        fig.colorbar(im, ax=ax, fraction=0.042, pad=0.03)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Run Visualization for the Dense Layer ---\n",
    "# We use the variables 'ref' and 'output' from the previous cell\n",
    "visualize_results(ref, output, title_prefix=\"Dense Layer\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMFAa9WXuf7KwcOo/8iTVSB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
